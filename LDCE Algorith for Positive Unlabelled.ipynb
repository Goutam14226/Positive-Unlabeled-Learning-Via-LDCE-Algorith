{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bNP9WEYBXZhm"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UmXVOpiSN4Ed"
      },
      "outputs": [],
      "source": [
        "#Defining The Covariance Matrix\n",
        "def Covariance_matrix(Sn_bar_features,Sn_bar_labels):\n",
        "  n=Sn_bar_features.shape[1]\n",
        "  k1=np.zeros((n,n))\n",
        "  for i in range(n):\n",
        "    x_i=Sn_bar_features[i].reshape(n,1)\n",
        "    x_i_transpose=x_i.T\n",
        "    k1=k1+x_i@x_i_transpose\n",
        "  k2=k1/n\n",
        "  k3=np.zeros((n,1))\n",
        "  k4=np.zeros((1,n))\n",
        "  for i in range(n):\n",
        "    z_i=Sn_bar_features[i].reshape(n,1)\n",
        "    y_i_bar=Sn_bar_labels[i]\n",
        "    k3=k3+z_i*y_i_bar\n",
        "    k4=k4+z_i.T*y_i_bar\n",
        "  k5=(k3@k4)/(n**3)\n",
        "  cov_matrix=k2-k5\n",
        "  return cov_matrix\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zS3zKn2jYMNw"
      },
      "outputs": [],
      "source": [
        "# gradient of the fucntion\n",
        "import numpy as np\n",
        "\n",
        "def objective_function(Sn, k,indices, w, eta, lambda_val, p,mu):\n",
        "    n=len(Sn)\n",
        "    d=Sn.shape[1]-1\n",
        "    c=-(n-k)/(2*n)\n",
        "    y=Sn[:,d]\n",
        "    x=Sn[:,0:d]\n",
        "    hinge_loss_sum_clean=0\n",
        "    hinge_loss_sum_noisy=0\n",
        "\n",
        "    for i in indices:\n",
        "        if i < k:\n",
        "            hinge_loss_sum_clean += max(0, 1 - y[i] * np.dot(x[i], w))\n",
        "\n",
        "    # Compute hinge loss for noisy examples (indices greater than or equal to k)\n",
        "    for i in indices:\n",
        "        if i >= k:\n",
        "            hinge_loss_sum_noisy += max(0, 1 - y[i] * np.dot(x[i], w))\n",
        "\n",
        "    # Regularization term\n",
        "    regularization_term = lambda_val * np.linalg.norm(w)**2\n",
        "    d= (1 - 2 * p * eta )\n",
        "\n",
        "    # Loss term for the noisy part\n",
        "    loss_noisy = c * np.dot(w, mu)/d + regularization_term\n",
        "\n",
        "    return (1/n) * (hinge_loss_sum_clean) + (1/(2*n)) * (hinge_loss_sum_noisy) + loss_noisy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FzNvYEiWP-tU"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def objective_function_gradient(Sn, k,indices, w, eta, lambda_val, p,mu):\n",
        "\n",
        "    n = len(Sn)\n",
        "    d = Sn.shape[1] - 1\n",
        "    c = -(n - k) / (2 * n)\n",
        "    y = Sn[:, d]\n",
        "    x=Sn[:,0:d]\n",
        "\n",
        "    # Gradient of the empirical risk term using hinge loss\n",
        "    hinge_loss_grad_clean = np.zeros_like(w).astype(float)\n",
        "    hinge_loss_grad_noisy = np.zeros_like(w).astype(float)\n",
        "\n",
        "    # Compute gradient for clean examples\n",
        "    for i in indices:\n",
        "        if i < k:\n",
        "            if 1 - y[i] * np.dot(x[i], w) > 0:\n",
        "                hinge_loss_grad_clean -= y[i] * x[i]\n",
        "        else:\n",
        "            if 1 - y[i] * np.dot(x[i], w) > 0:\n",
        "                hinge_loss_grad_noisy -= y[i] * x[i]\n",
        "\n",
        "    # Regularization term gradient\n",
        "    regularization_term_grad = 2 * lambda_val * w\n",
        "\n",
        "    # Gradient of the loss term for the noisy part\n",
        "    d=d= (1 - 2 * p * eta )\n",
        "    loss_noisy_grad = (c /d)*mu\n",
        "\n",
        "    # Combine gradients\n",
        "    gradient = (1 / n) * hinge_loss_grad_clean + (1 / (2 * n)) * hinge_loss_grad_noisy + loss_noisy_grad + regularization_term_grad\n",
        "\n",
        "    return gradient\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1lfhn-xyh3tx"
      },
      "outputs": [],
      "source": [
        "def exact_line_search(f, gradient,indices,x,alpha, beta,Sn,k,eta,lambda_val,p,mu, tol=1e-3, max_iter=100):\n",
        "    a = 0.0\n",
        "    b = 1.0\n",
        "\n",
        "    iter_count = 0\n",
        "    while iter_count < max_iter:\n",
        "        # Compute function values at points a and b\n",
        "        gr=gradient(Sn, k,indices, x, eta, lambda_val, p,mu)\n",
        "        fa = f(Sn, k,indices, x-a*gr, eta, lambda_val, p,mu)\n",
        "        fb = f(Sn, k, indices,x-b*gr, eta, lambda_val, p,mu)\n",
        "\n",
        "        # Check if the interval is small enough\n",
        "        if b - a < tol:\n",
        "            break\n",
        "\n",
        "        # Compute the midpoint of the interval\n",
        "        t = (a + b) / 2\n",
        "\n",
        "        # Compute function value at the midpoint\n",
        "        ft = f(Sn, k, indices,x-t*gr, eta, lambda_val, p,mu)\n",
        "\n",
        "        # Update the interval based on function values\n",
        "        if ft < fa and ft < fb:\n",
        "            b = t\n",
        "        else:\n",
        "            a = t\n",
        "\n",
        "        iter_count += 1\n",
        "\n",
        "    # Return the midpoint of the final interval as the optimal step size\n",
        "    return (a + b) / 2\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1lsX7Rk0HNeD"
      },
      "outputs": [],
      "source": [
        "def median_of_mean(Sn_bar,g):\n",
        "  # split the data into g equal parts\n",
        "  num_parts=len(Sn_bar)//g\n",
        "  groups=[Sn_bar[i*(num_parts):(i+1)*num_parts,:] for i in range(g)]\n",
        "\n",
        "  # calcuate the mean of each part and store it in a list\n",
        "  means = [np.mean(part, axis=0) for part in groups]\n",
        "\n",
        "  # calculate the differences and store it in a list for each part\n",
        "  differences=[]\n",
        "  for i , mean_i in enumerate(means):\n",
        "    diff=[]\n",
        "    for j, mean_j in enumerate(means):\n",
        "      if i!=j:\n",
        "        diff.append(np.abs(mean_i-mean_j))\n",
        "    differences.append(diff)\n",
        "\n",
        "  # store the medians for each group in a list\n",
        "  median_list=[]\n",
        "  for arr in differences:\n",
        "    med_arr=[]\n",
        "    for ele in arr:\n",
        "      me=np.median(ele)\n",
        "      med_arr.append(me)\n",
        "    med_final=np.median(med_arr)\n",
        "    median_list.append(med_final)\n",
        "  for i , median in enumerate(median_list):\n",
        "    if median==min(median_list):\n",
        "      gr_oup=i\n",
        "      break\n",
        "  u_Sn=means[i]\n",
        "  return u_Sn\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def stochastic_gradient_descent_armijo(func, grad, x0, Sn, k, eta, lambda_val, mu, p, beta, tol, batch_size=50, alpha=0.5,  max_iter=1000):\n",
        "    iter_count = 0\n",
        "    x = x0\n",
        "\n",
        "    while iter_count < max_iter:\n",
        "        # Randomly select a mini-batch\n",
        "        indices = np.random.choice(len(Sn), batch_size, replace=False)\n",
        "\n",
        "\n",
        "        # Compute the gradient at the current point using the mini-batch\n",
        "        grad_x = grad(Sn, k, indices, x, eta, lambda_val, p, mu)\n",
        "\n",
        "        # Compute the search direction (negative gradient)\n",
        "        d = -grad_x\n",
        "\n",
        "        # Determine the step size using Armijo condition\n",
        "        t = exact_line_search(func, grad, indices,x, alpha, beta, Sn, k, eta, lambda_val, p, mu, tol=1e-3, max_iter=100)\n",
        "\n",
        "        # Update the variable using the step size\n",
        "        x_new = x + t * d\n",
        "        iter_count += 1\n",
        "\n",
        "        # Check for convergence\n",
        "        if np.linalg.norm(x_new - x) < tol:\n",
        "            break\n",
        "\n",
        "        x = x_new\n",
        "\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "TCzVxgc1aDUB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kaOlnyLC1mj2"
      },
      "outputs": [],
      "source": [
        "# Loss Decomposition And Centroid Estimation Algorithm\n",
        "def LDCE_algorithm(Sn_bar,eta,lambda_val,beta,k):\n",
        "  #calling algorithm 1 to give mucap\n",
        "  n=len(Sn_bar)\n",
        "  d=Sn_bar.shape[1]-1\n",
        "  label=Sn_bar[:,d]\n",
        "  feature_space=Sn_bar[:,0:d]\n",
        "  Corr_sample=feature_space[k:]\n",
        "  corr_label=label[k:]\n",
        "  g=5\n",
        "  mu_bar=median_of_mean(Corr_sample,g)\n",
        "  # Calculating The covariance MAtrix\n",
        "  cov_matrix=Covariance_matrix(Corr_sample,corr_label)\n",
        "  t=0\n",
        "  w0=np.zeros(d)\n",
        "  # Calculating Mu\n",
        "  try:\n",
        "    cov_inverse = np.linalg.inv(cov_matrix)\n",
        "\n",
        "  except np.linalg.LinAlgError:\n",
        "\n",
        "    cov_inverse = np.linalg.pinv(cov_matrix)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  w=w0.reshape(d,1)\n",
        "  epsilon = 1e-5\n",
        "  denominator = w.T @ cov_inverse @ w\n",
        "  denominator_safe = np.maximum(denominator, epsilon)\n",
        "  term = np.sqrt(beta / denominator_safe)\n",
        "  final_mu = mu_bar + cov_inverse @ w0 * term\n",
        "\n",
        "  f_u=np.squeeze(final_mu)\n",
        "  fun=objective_function\n",
        "  gra=objective_function_gradient\n",
        "  p=k/(n*(1-eta))\n",
        "  tol=1e-1\n",
        "  k1=k\n",
        "  converged_w=stochastic_gradient_descent_armijo(fun,gra,w0,Sn_bar,k1,eta, lambda_val, f_u, p, beta,tol,batch_size=50, alpha=0.5, max_iter=1000)\n",
        "  return converged_w\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_openml\n",
        "\n",
        "# Load USPS handwritten digit dataset\n",
        "usps = fetch_openml('usps', version=2)\n",
        "\n",
        "# Extract features and target labels\n",
        "X = usps.data\n",
        "y = usps.target\n",
        "\n",
        "print(\"Shape of features:\", X.shape)\n",
        "print(\"Shape of target labels:\", y.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S5NqqN79lkjm",
        "outputId": "e16711eb-baec-41d8-f8ae-1e4621695908"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/datasets/_openml.py:968: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
            "  warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of features: (9298, 256)\n",
            "Shape of target labels: (9298,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xY9_Sq9hcQ37"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "ar1=X.values\n",
        "ar2=y.values\n",
        "arr_label = np.concatenate((ar1, ar2.reshape(-1,1)), axis=1)\n"
      ],
      "metadata": {
        "id": "qn4_aHJkm576"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Positive1=[]\n",
        "negative1=[]\n",
        "for i in range(len(X)):\n",
        "  if arr_label[i,256]=='1':\n",
        "    Positive1.append(arr_label[i])\n",
        "  else:\n",
        "    negative1.append(arr_label[i])"
      ],
      "metadata": {
        "id": "SFXwwb86vBFd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for neta to be 0.2 the value of k came out to be\n",
        "k_ups=int(1553-0.2*1553)\n",
        "# Extracting out Only features\n",
        "final_ups_array=np.vstack(Positive1+negative1)\n",
        "final_ups_features=final_ups_array[:,0:final_ups_array.shape[1]-1]"
      ],
      "metadata": {
        "id": "cyaj6k8sxJD6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corr_label_list1=[-1]*k_ups +[1]*(len(X)-k_ups)\n",
        "corr_label_array1=np.array(corr_label_list1).reshape(-1,1)\n",
        "corr_data_ups=np.hstack((final_ups_features,corr_label_array1))\n",
        "corr_data_ups = corr_data_ups.astype(int)\n"
      ],
      "metadata": {
        "id": "1e7dciKFx49q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corr_data_ups[:,corr_data_ups.shape[1]-1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88-C32IIDSpA",
        "outputId": "502c0309-755d-4daa-9706-03b3f6f13140"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 1,  1,  1, ..., -1, -1, -1])"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corr_label_list1=[-1]*k_ups +[1]*(len(X)-k_ups)"
      ],
      "metadata": {
        "id": "DjfJ9jEdvZJx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y_true_label=np.array(corr_label_list1)"
      ],
      "metadata": {
        "id": "pAHbBCyJvfvG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ups_corr=corr_data_ups\n",
        "eta1=0.2\n",
        "lambda_vall=0.5\n",
        "beta1=0.5\n",
        "\n",
        "weight_ups=LDCE_algorithm(ups_corr,eta1,lambda_vall,beta1,k_ups)"
      ],
      "metadata": {
        "id": "RGfqTw7B0A1k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def classify_data_points(X, w):\n",
        "    # Classify data points based on the sign of the dot product between feature vector and weight vector\n",
        "    y_pred = np.sign(np.dot(X, w))\n",
        "    return y_pred\n",
        "\n",
        "def compute_f1_score(y_true, y_pred):\n",
        "    # Compute true positives, false positives, and false negatives\n",
        "    TP = np.sum((y_true == 1) & (y_pred == 1))\n",
        "    FP = np.sum((y_true == -1) & (y_pred == 1))\n",
        "    FN = np.sum((y_true == 1) & (y_pred == -1))\n",
        "\n",
        "    # Compute precision and recall\n",
        "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
        "    recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
        "\n",
        "    # Compute F1 score\n",
        "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "    return f1_score\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Example weight vector\n",
        "    w = weight_ups  # Example weight vector\n",
        "\n",
        "    # Example data points\n",
        "    X = corr_data_ups[:,0:corr_data_ups.shape[1]-1]\n",
        "\n",
        "    # Example true labels\n",
        "    y_true = Y_true_label\n",
        "\n",
        "    # Classify data points\n",
        "    y_pred = classify_data_points(X, w)\n",
        "\n",
        "    # Compute F1 score\n",
        "    f1_score = compute_f1_score(y_true, y_pred)\n",
        "    print(\"F1 Score:\", f1_score)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i0OtOaGrs8Jw",
        "outputId": "8464f1d7-4619-4ef8-cf9b-856edf79430c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 Score: 0.9390825688073395\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/balance-scale/balance-scale.data\"\n",
        "column_names = [\"Class\", \"Left-Weight\", \"Left-Distance\", \"Right-Weight\", \"Right-Distance\"]\n",
        "data = pd.read_csv(url, names=column_names)\n",
        "\n",
        "# Display the first few rows of the dataset\n",
        "print(data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LGIg6fdTwyTD",
        "outputId": "aa9ede47-b8ab-421e-b936-de7df199e105"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    Class  Left-Weight  Left-Distance  Right-Weight  Right-Distance\n",
            "0       B            1              1             1               1\n",
            "1       R            1              1             1               2\n",
            "2       R            1              1             1               3\n",
            "3       R            1              1             1               4\n",
            "4       R            1              1             1               5\n",
            "..    ...          ...            ...           ...             ...\n",
            "620     L            5              5             5               1\n",
            "621     L            5              5             5               2\n",
            "622     L            5              5             5               3\n",
            "623     L            5              5             5               4\n",
            "624     B            5              5             5               5\n",
            "\n",
            "[625 rows x 5 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "balance_data=data.values"
      ],
      "metadata": {
        "id": "TQJYDPKK0Utk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "balance_data_features=balance_data[:,1:data.shape[1]]"
      ],
      "metadata": {
        "id": "aRhvRJ011IyA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "\n",
        "\n",
        "# Create a MinMaxScaler object\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# Fit the scaler to your data and transform it\n",
        "normalized_data = scaler.fit_transform(balance_data_features)\n",
        "normalized_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oLt14oP31TAC",
        "outputId": "9495af65-3b9b-4eeb-fbb1-4c1bc3697388"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.  , 0.  , 0.  , 0.  ],\n",
              "       [0.  , 0.  , 0.  , 0.25],\n",
              "       [0.  , 0.  , 0.  , 0.5 ],\n",
              "       ...,\n",
              "       [1.  , 1.  , 1.  , 0.5 ],\n",
              "       [1.  , 1.  , 1.  , 0.75],\n",
              "       [1.  , 1.  , 1.  , 1.  ]])"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "normalized_data[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vQsyadKf2snS",
        "outputId": "da12e578-1cea-4604-8321-1dda92d5f7b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.  , 0.  , 0.  , 0.25])"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "l1=data['Class'].tolist()"
      ],
      "metadata": {
        "id": "mq4yNs6fw1zF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Positive2=[]\n",
        "Negative2=[]\n"
      ],
      "metadata": {
        "id": "_0IfRO3s2lz7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Positive2=[]\n",
        "Negative2=[]\n",
        "for i in range(len(l1)):\n",
        "  if l1[i]==\"R\":\n",
        "    Positive2.append(normalized_data[i])\n",
        "  else:\n",
        "    Negative2.append(normalized_data[i])\n"
      ],
      "metadata": {
        "id": "xtyvDsHjy_Pk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eta=0.2\n",
        "k_balance=int(288-0.2*288)\n"
      ],
      "metadata": {
        "id": "EGl5n0nj0LvR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Creating the corrupted version of the dataset"
      ],
      "metadata": {
        "id": "TTuXueha32zk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corr_label_bal=[1]*k_balance + [-1]*(len(data)-k_balance)\n",
        "corr_bal=np.vstack(Positive2 + Negative2)\n",
        "corr_label_bal1=np.array(corr_label_bal).reshape(-1,1)\n",
        "\n"
      ],
      "metadata": {
        "id": "XfFUqjsN37Po"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corr_data_balance=np.hstack((corr_bal,corr_label_bal1))"
      ],
      "metadata": {
        "id": "q7ENiDJZ4wYn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bal_corr=corr_data_balance\n",
        "eta1=0.2\n",
        "lambda_vall=0.5\n",
        "beta1=0.5\n",
        "\n",
        "weight_balance=LDCE_algorithm(bal_corr,eta1,lambda_vall,beta1,k_balance)"
      ],
      "metadata": {
        "id": "ZY2Igpso7xWI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def accuracy_data(data,weigh):\n",
        "  s1=0\n",
        "  s2=0\n",
        "  n=len(data)\n",
        "  for i in range(288):\n",
        "\n",
        "    f=data[i]\n",
        "    if np.dot(weigh,f)>=0:\n",
        "      s1+=1\n",
        "  for i in range(288,n):\n",
        "\n",
        "    f=data[i]\n",
        "    if np.dot(weigh,f)<=0:\n",
        "      s2+=1\n",
        "  accur=(s1+s2)/(n)\n",
        "  return accur"
      ],
      "metadata": {
        "id": "tKg4dloY7zhg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Example weight vector\n",
        "    w1 = weight_balance  # Example weight vector\n",
        "\n",
        "    # Example data points\n",
        "    X1 = corr_data_balance[:,0:corr_data_balance.shape[1]-1]\n",
        "\n",
        "    # Example true labels\n",
        "    y_true1 = corr_data_balance[:,corr_data_balance.shape[1]-1]\n",
        "\n",
        "    # Classify data points\n",
        "    y_pred1 = classify_data_points(X1, w1)\n",
        "\n",
        "    # Compute F1 score\n",
        "    f1_score1 = compute_f1_score(y_true1, y_pred1)\n",
        "    print(\"F1 Score for balance dataset:\", f1_score1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oDg_FNcq82iG",
        "outputId": "a9e6317b-1223-4055-d403-6b76e51350f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 Score for balance dataset: 0.5386416861826698\n"
          ]
        }
      ]
    }
  ]
}